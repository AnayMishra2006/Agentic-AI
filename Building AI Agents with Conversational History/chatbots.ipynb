{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65ad41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e236fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000002C32B5CBE00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002C32B770C20>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-20b\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Anay! üëãüèº It‚Äôs great to meet an Agentic AI learner. How can I support you today? Are you working on a specific project, exploring a particular framework, or just looking for general guidance on building autonomous agents? Let me know what you‚Äôre interested in, and we can dive right in!', additional_kwargs={'reasoning_content': 'User says \"Hi, My name is Anay and I am a Agentic AI learner\". They are likely greeting. We need to respond politely, maybe ask about their goals. Provide helpful info.'}, response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 86, 'total_tokens': 201, 'completion_time': 0.117112923, 'completion_tokens_details': {'reasoning_tokens': 41}, 'prompt_time': 0.004084915, 'prompt_tokens_details': None, 'queue_time': 0.156226965, 'total_time': 0.121197838}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a8c584dda7', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6aa3-3664-7c11-991a-893bafa8c2f8-0', usage_metadata={'input_tokens': 86, 'output_tokens': 115, 'total_tokens': 201, 'output_token_details': {'reasoning': 41}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Anay and I am a Agentic AI learner\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You‚Äôre Anay, and you‚Äôre learning to build and work with Agentic AI systems‚Äîautonomous agents that can plan, act, and adapt to achieve goals. üöÄ If you‚Äôd like to dive deeper into any specific aspect (like architecture, tools, or project ideas), just let me know!', additional_kwargs={'reasoning_content': 'User says: \"Hey Whats my name and what do i do?\" They previously said: \"Hi, My name is Anay and I am a Agentic AI learner\". So we should respond with name and what they do: Anay, Agentic AI learner. Also maybe ask more.'}, response_metadata={'token_usage': {'completion_tokens': 131, 'prompt_tokens': 171, 'total_tokens': 302, 'completion_time': 0.147937799, 'completion_tokens_details': {'reasoning_tokens': 60}, 'prompt_time': 0.009200233, 'prompt_tokens_details': None, 'queue_time': 0.047651907, 'total_time': 0.157138032}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6aa6-3cac-7271-aa84-c0006a3d1bfb-0', usage_metadata={'input_tokens': 171, 'output_tokens': 131, 'total_tokens': 302, 'output_token_details': {'reasoning': 60}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Anay and I am a Agentic AI learner\"),\n",
    "        AIMessage(content=\"Hello Anay! üëãüèº It‚Äôs great to meet an Agentic AI learner. How can I support you today? Are you working on a specific project, exploring a particular framework, or just looking for general guidance on building autonomous agents? Let me know what you‚Äôre interested in, and we can dive right in!\"),\n",
    "        HumanMessage(content=\"Hey Whats my name and what do i do?\")\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5505df3",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message Histroy class to wrap our model and make it stateful. This will keep track of outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Anay and I am an Agentic AI learner\")\n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Anay! üëã I‚Äôm excited to chat with an Agentic AI learner. What‚Äôs on your mind today? Are you exploring a particular AI topic, working on a project, or just curious about the latest developments? Let me know how I can help!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5559be96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I‚Äôm not sure‚Äîcould you let me know what you‚Äôd like me to call you?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config that means changing the session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3434eb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, John! How can I help you today?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey my name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "429b6650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8874d9",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt templates helps to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions. Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2037b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the questions to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "977ca8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Anay! üëã How can I help you today?', additional_kwargs={'reasoning_content': 'We need to respond politely, acknowledging the user, asking how can help.'}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 96, 'total_tokens': 134, 'completion_time': 0.040772774, 'completion_tokens_details': {'reasoning_tokens': 16}, 'prompt_time': 0.006389954, 'prompt_tokens_details': None, 'queue_time': 0.354236675, 'total_time': 0.047162728}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6ac9-0bc0-7bc1-a0ce-0c54081f4a16-0', usage_metadata={'input_tokens': 96, 'output_tokens': 38, 'total_tokens': 134, 'output_token_details': {'reasoning': 16}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Anay\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcf2765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6359d46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Anay! Nice to meet you. How can I help you today?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Anay\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d858461",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61b80454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§Ö‡§®‡§æ‡§Ø! ‡§Ü‡§™‡§∏‡•á ‡§Æ‡§ø‡§≤‡§ï‡§∞ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•Å‡§à‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡§ø‡§∏ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ/‡§∏‡§ï‡§§‡•Ä ‡§π‡•Ç‡§Å?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Anay\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041b85d",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c19d9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fb52362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã ‡¶Ü‡¶®‡¶æ‡¶Ø‡¶º! ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {'messages':[HumanMessage(content=\"Hi, I am Anay\")],'language':\"Bengali\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b089a",
   "metadata": {},
   "source": [
    "### Managing the conversation history\n",
    "\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messgaes you are passing in.\n",
    "\n",
    "'trim_messages'  ->> helps to reduce how many messages we're sending to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8daab0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like butterscotch ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"what's 2+2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Anytime', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hell yeah!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=50,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages=[\n",
    "    SystemMessage(content=\"You're a good assistant\"),\n",
    "    HumanMessage(content=\"Hi! I am Anay\"),\n",
    "    AIMessage(content=\"HI!\"),\n",
    "    HumanMessage(content=\"I like butterscotch ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"what's 2+2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Thanks!\"),\n",
    "    AIMessage(content=\"Anytime\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"Hell yeah!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a28699b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I‚Äôm not sure‚Äîwhat‚Äôs your favorite flavor? If you tell me a bit about what you like (sweet, fruity, chocolatey, creamy, adventurous, etc.), I can suggest something that might hit the spot!', additional_kwargs={'reasoning_content': 'User asks \"What ice cream do i like\". We don\\'t know preferences. Should respond asking for preferences.'}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 155, 'total_tokens': 231, 'completion_time': 0.083164902, 'completion_tokens_details': {'reasoning_tokens': 22}, 'prompt_time': 0.008741506, 'prompt_tokens_details': None, 'queue_time': 0.125816054, 'total_time': 0.091906408}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6aee-7e3e-7262-9cce-58831d3bb435-0', usage_metadata={'input_tokens': 155, 'output_tokens': 76, 'total_tokens': 231, 'output_token_details': {'reasoning': 22}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "379d5689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked for the simple arithmetic problem ‚Äú2\\u202f+\\u202f2.‚Äù  \\n**Answer:** 4.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What math problem did i ask for and answer it too\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a428b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's wrap this in the message history\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbda053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don‚Äôt have that information. If you‚Äôd like to share it, I‚Äôll remember it for our conversation!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content=\"What's my name?\")],\n",
    "        \"language\":\"english\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
